{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Regresão base de dados Microsoft**\n",
        "\n",
        "**Guilherme Henrique Pereira Serafini - 2021.1.08.048**\n",
        "\n",
        "\n",
        "**Vinícius Eduardo de Souza Honório - 2021.1.08.024**"
      ],
      "metadata": {
        "id": "HDM6GM6wBlc2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_gPc7EMBi_w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "094149ae-566e-4d54-9bb3-8b02ebf98954"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modelos selecionados:\n",
            "Random Forest - R2: 0.9995, MSE: 86.3547\n",
            "Random Forest - R2: 0.9994, MSE: 90.1847\n",
            "Random Forest - R2: 0.9994, MSE: 90.3975\n",
            "Random Forest - R2: 0.9994, MSE: 90.4036\n",
            "Random Forest - R2: 0.9994, MSE: 90.5811\n",
            "Random Forest - R2: 0.9994, MSE: 91.4553\n",
            "Random Forest - R2: 0.9994, MSE: 92.2941\n",
            "Random Forest - R2: 0.9994, MSE: 92.6480\n",
            "Random Forest - R2: 0.9994, MSE: 93.8475\n",
            "XGBoost - R2: 0.9994, MSE: 95.0645\n",
            "XGBoost - R2: 0.9994, MSE: 95.0645\n",
            "XGBoost - R2: 0.9994, MSE: 95.0645\n",
            "XGBoost - R2: 0.9994, MSE: 95.0645\n",
            "Random Forest - R2: 0.9994, MSE: 95.8904\n",
            "XGBoost - R2: 0.9994, MSE: 96.8609\n",
            "XGBoost - R2: 0.9994, MSE: 96.8609\n",
            "XGBoost - R2: 0.9994, MSE: 96.8609\n",
            "XGBoost - R2: 0.9994, MSE: 96.8609\n",
            "Random Forest - R2: 0.9994, MSE: 96.9139\n",
            "Random Forest - R2: 0.9994, MSE: 97.3079\n",
            "Random Forest - R2: 0.9994, MSE: 97.6190\n",
            "Random Forest - R2: 0.9994, MSE: 98.0767\n",
            "Random Forest - R2: 0.9994, MSE: 99.6165\n",
            "Random Forest - R2: 0.9994, MSE: 100.2553\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "\n",
        "# Carregar os dados\n",
        "data = pd.read_csv('https://raw.githubusercontent.com/megaVE/LearningVerification2/main/Microsoft_Stock.csv')\n",
        "columns = [ \"Date\", \"Volume\"] + [f'Feature_{i}' for i in range(1, 5)]\n",
        "data.columns = columns\n",
        "\n",
        "data = data.dropna()\n",
        "\n",
        "X = data.drop(['Date', 'Volume'], axis=1)\n",
        "y = data['Volume']\n",
        "\n",
        "X = StandardScaler().fit_transform(X)\n",
        "y = LabelEncoder().fit_transform(y)\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Dividir o conjunto de treinamento em 4 subconjuntos\n",
        "X_train_subconjuntos = []\n",
        "y_train_subconjuntos = []\n",
        "for _ in range(4):\n",
        "    X_subconjunto, _, y_subconjunto, _ = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "    X_train_subconjuntos.append(X_subconjunto)\n",
        "    y_train_subconjuntos.append(y_subconjunto)\n",
        "\n",
        "#################################################################################################################\n",
        "random_forest_params_variations = []\n",
        "while len(random_forest_params_variations) < 15:\n",
        "    new_params_forest = {\n",
        "      'n_estimators': np.random.choice([1, 3, 5]),\n",
        "      'max_depth': np.random.choice([None, 5, 10]),\n",
        "      'min_samples_split': np.random.choice([2, 5, 10])\n",
        "    }\n",
        "    if new_params_forest not in random_forest_params_variations:\n",
        "        random_forest_params_variations.append(new_params_forest)\n",
        "\n",
        "\n",
        "random_forest_modelos = []\n",
        "for variation in random_forest_params_variations:\n",
        "    random_forest_model = RandomForestRegressor(**variation)\n",
        "    random_forest_model1 = RandomForestRegressor(**variation)\n",
        "    random_forest_model2 = RandomForestRegressor(**variation)\n",
        "    random_forest_model3 = RandomForestRegressor(**variation)\n",
        "    random_forest_model.fit(X_train_subconjuntos[0], y_train_subconjuntos[0])\n",
        "    random_forest_model1.fit(X_train_subconjuntos[1], y_train_subconjuntos[1])\n",
        "    random_forest_model2.fit(X_train_subconjuntos[2], y_train_subconjuntos[2])\n",
        "    random_forest_model3.fit(X_train_subconjuntos[3], y_train_subconjuntos[3])\n",
        "    random_forest_modelos.append(random_forest_model)\n",
        "    random_forest_modelos.append(random_forest_model1)\n",
        "    random_forest_modelos.append(random_forest_model2)\n",
        "    random_forest_modelos.append(random_forest_model3)\n",
        "\n",
        "xgb_params_variations = []\n",
        "while len(xgb_params_variations) < 15:\n",
        "    new_params_xgb = {\n",
        "        'n_estimators': np.random.choice([50, 100, 200]),\n",
        "        'learning_rate': np.random.choice([0.1, 0.01, 0.001]),\n",
        "        'max_depth': np.random.choice([3, 5, 7, 9])\n",
        "    }\n",
        "    if new_params_xgb not in xgb_params_variations:\n",
        "        xgb_params_variations.append(new_params_xgb)\n",
        "\n",
        "\n",
        "xgb_modelos = []\n",
        "for variation in xgb_params_variations:\n",
        "    model = XGBRegressor(**variation)\n",
        "    model1 = XGBRegressor(**variation)\n",
        "    model2 = XGBRegressor(**variation)\n",
        "    model3 = XGBRegressor(**variation)\n",
        "\n",
        "    model.fit(X_train_subconjuntos[0], y_train_subconjuntos[0])\n",
        "    model1.fit(X_train_subconjuntos[1], y_train_subconjuntos[1])\n",
        "    model2.fit(X_train_subconjuntos[2], y_train_subconjuntos[2])\n",
        "    model3.fit(X_train_subconjuntos[3], y_train_subconjuntos[3])\n",
        "\n",
        "    xgb_modelos.append(model)\n",
        "    xgb_modelos.append(model1)\n",
        "    xgb_modelos.append(model2)\n",
        "    xgb_modelos.append(model3)\n",
        "\n",
        "svr_params_variations = []\n",
        "while len(svr_params_variations) < 15:\n",
        "    svr_params = {\n",
        "        'C': np.random.choice([0.1, 1.0, 10.0]),\n",
        "        'kernel': np.random.choice(['linear', 'rbf', 'poly']),\n",
        "        'degree': np.random.choice([2, 3, 4]),\n",
        "    }\n",
        "    if svr_params not in svr_params_variations:\n",
        "        svr_params_variations.append(svr_params)\n",
        "\n",
        "svr_modelos = []\n",
        "for variation in svr_params_variations:\n",
        "    model = SVR(**variation)\n",
        "    model1 = SVR(**variation)\n",
        "    model2 = SVR(**variation)\n",
        "    model3 = SVR(**variation)\n",
        "\n",
        "    model.fit(X_train_subconjuntos[0], y_train_subconjuntos[0])\n",
        "    model1.fit(X_train_subconjuntos[1], y_train_subconjuntos[1])\n",
        "    model2.fit(X_train_subconjuntos[2], y_train_subconjuntos[2])\n",
        "    model3.fit(X_train_subconjuntos[3], y_train_subconjuntos[3])\n",
        "\n",
        "    svr_modelos.append(model)\n",
        "    svr_modelos.append(model1)\n",
        "    svr_modelos.append(model2)\n",
        "    svr_modelos.append(model3)\n",
        "\n",
        "decision_tree_params_variations = []\n",
        "while len(decision_tree_params_variations) < 15:\n",
        "    decision_tree_params = {\n",
        "      'max_depth': np.random.choice([None, 5, 10]),\n",
        "      'min_samples_split': np.random.choice([2, 5, 10]),\n",
        "      'min_samples_leaf': np.random.choice([1, 2, 4])\n",
        "    }\n",
        "    if decision_tree_params not in decision_tree_params_variations:\n",
        "        decision_tree_params_variations.append(decision_tree_params)\n",
        "\n",
        "decision_tree_modelos = []\n",
        "for variation in decision_tree_params_variations:\n",
        "    model = DecisionTreeRegressor(**variation)\n",
        "    model1 = DecisionTreeRegressor(**variation)\n",
        "    model2 = DecisionTreeRegressor(**variation)\n",
        "    model3 = DecisionTreeRegressor(**variation)\n",
        "\n",
        "    model.fit(X_train_subconjuntos[0], y_train_subconjuntos[0])\n",
        "    model1.fit(X_train_subconjuntos[1], y_train_subconjuntos[1])\n",
        "    model2.fit(X_train_subconjuntos[2], y_train_subconjuntos[2])\n",
        "    model3.fit(X_train_subconjuntos[3], y_train_subconjuntos[3])\n",
        "\n",
        "    decision_tree_modelos.append(model)\n",
        "    decision_tree_modelos.append(model1)\n",
        "    decision_tree_modelos.append(model2)\n",
        "    decision_tree_modelos.append(model3)\n",
        "\n",
        "model_accuracies = []\n",
        "# Loop para aplicar cada modelo aos dados de teste e obter as métricas de regressão\n",
        "for model in random_forest_modelos + xgb_modelos + svr_modelos + decision_tree_modelos:\n",
        "    y_pred = model.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    model_accuracies.append((mse, r2))\n",
        "\n",
        "top_indices = sorted(range(len(model_accuracies)), key=lambda i: model_accuracies[i][1], reverse=True)[:24]\n",
        "\n",
        "modelos_melhores = []\n",
        "\n",
        "modelos_lista = []\n",
        "\n",
        "for i in top_indices:\n",
        "    if i < len(random_forest_modelos):\n",
        "        model_name = 'Random Forest'\n",
        "        model = random_forest_modelos[i]\n",
        "    elif i < len(random_forest_modelos) + len(xgb_modelos):\n",
        "        model_name = 'XGBoost'\n",
        "        model = xgb_modelos[i - len(random_forest_modelos)]\n",
        "    elif i < len(random_forest_modelos) + len(xgb_modelos) + len(svr_modelos):\n",
        "        model_name = 'SVR'\n",
        "        model = svr_modelos[i - len(random_forest_modelos) - len(xgb_modelos)]\n",
        "    else:\n",
        "        model_name = 'Decision Tree'\n",
        "        model = decision_tree_modelos[i - len(random_forest_modelos) - len(xgb_modelos) - len(svr_modelos)]\n",
        "\n",
        "    modelos_melhores.append((model_name, model_accuracies[i][1], model_accuracies[i][0]))\n",
        "    modelos_lista.append(model)\n",
        "\n",
        "# Imprime os modelos selecionados e suas métricas\n",
        "print(\"Modelos selecionados:\")\n",
        "for model_name, r2, mse in modelos_melhores:\n",
        "    print(f\"{model_name} - R2: {r2:.4f}, MSE: {mse:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicializar uma matriz para armazenar as previsões de cada modelo\n",
        "previsoes_modelos = np.zeros((len(modelos_lista), len(X_test)))\n",
        "\n",
        "# Fazer previsões para cada modelo em X_teste\n",
        "for i, modelo in enumerate(modelos_lista):\n",
        "    previsoes_modelos[i] = modelo.predict(X_test)\n",
        "\n",
        "# Calcular a média das previsões de todos os modelos para obter a previsão final\n",
        "previsao_final = np.mean(previsoes_modelos, axis=0)\n",
        "\n",
        "# Compare as previsões finais com os valores reais em y_teste\n",
        "acuracia_ensemble = r2_score(y_test, previsao_final)\n",
        "print(\"Metrica de erro quadratico do ensemble usando a média:\", acuracia_ensemble)\n"
      ],
      "metadata": {
        "id": "Uf1OKs50BpAO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55206322-b1fa-4aab-83fd-a22442518bbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metrica de erro quadratico do ensemble usando a média: 0.9994870996187853\n"
          ]
        }
      ]
    }
  ]
}
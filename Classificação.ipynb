{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Classificação base de cancêr de mama**\n",
        "\n",
        "**Guilherme Henrique Pereira Serafini - 2021.1.08.048**\n",
        "\n",
        "\n",
        "**Vinícius Eduardo de Souza Honório - 2021.1.08.024**"
      ],
      "metadata": {
        "id": "0SjXtvBG6tuW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6lO-yFA-1n4t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a81c93e-9986-476a-d8f0-54bc32aa89b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modelos selecionados:\n",
            "SVM : 0.9766\n",
            "SVM : 0.9766\n",
            "SVM : 0.9766\n",
            "SVM : 0.9766\n",
            "SVM : 0.9766\n",
            "SVM : 0.9766\n",
            "SVM : 0.9766\n",
            "SVM : 0.9766\n",
            "SVM : 0.9708\n",
            "SVM : 0.9708\n",
            "SVM : 0.9708\n",
            "SVM : 0.9708\n",
            "SVM : 0.9708\n",
            "SVM : 0.9708\n",
            "SVM : 0.9708\n",
            "SVM : 0.9708\n",
            "SVM : 0.9649\n",
            "SVM : 0.9649\n",
            "SVM : 0.9649\n",
            "SVM : 0.9649\n",
            "SVM : 0.9649\n",
            "SVM : 0.9649\n",
            "SVM : 0.9649\n",
            "SVM : 0.9649\n"
          ]
        }
      ],
      "source": [
        "# importar os pacotes necessários\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from sklearn.svm import SVC\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from collections import Counter\n",
        "\n",
        "# Carregar os dados\n",
        "data = pd.read_csv('https://raw.githubusercontent.com/megaVE/LearningVerification2/main/wdbc.csv')\n",
        "columns = ['ID', 'Diagnosis'] + [f'Feature_{i}' for i in range(1, 31)]\n",
        "data.columns = columns\n",
        "\n",
        "\n",
        "data = data.dropna()\n",
        "\n",
        "X = data.drop(['ID', 'Diagnosis'], axis=1)\n",
        "y = data['Diagnosis']\n",
        "\n",
        "# padronizar as colunas numéricas\n",
        "X = StandardScaler().fit_transform(X)\n",
        "\n",
        "# label encoder na variável alvo\n",
        "y = LabelEncoder().fit_transform(y)\n",
        "\n",
        "# dividir o dataset entre treino e teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "\n",
        "# Dividir o conjunto de treinamento em 4 subconjuntos\n",
        "X_train_subconjuntos = []\n",
        "y_train_subconjuntos = []\n",
        "for _ in range(4):\n",
        "    X_subconjunto, _, y_subconjunto, _ = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "    X_train_subconjuntos.append(X_subconjunto)\n",
        "    y_train_subconjuntos.append(y_subconjunto)\n",
        "\n",
        "###################################################################################################\n",
        "random_forest_params_variations = []\n",
        "while len(random_forest_params_variations) < 15:\n",
        "    new_params_forest = {\n",
        "      'n_estimators': np.random.choice([100, 300, 500]),\n",
        "      'max_depth': np.random.choice([None, 5, 10]),\n",
        "      'min_samples_split': np.random.choice([2, 5, 10])\n",
        "    }\n",
        "    if new_params_forest not in random_forest_params_variations:\n",
        "        random_forest_params_variations.append(new_params_forest)\n",
        "\n",
        "\n",
        "random_forest_modelos = []\n",
        "for variation in random_forest_params_variations:\n",
        "  random_forest_model = RandomForestClassifier(**variation)\n",
        "  random_forest_model1 = RandomForestClassifier(**variation)\n",
        "  random_forest_model2 = RandomForestClassifier(**variation)\n",
        "  random_forest_model3 = RandomForestClassifier(**variation)\n",
        "  random_forest_model.fit(X_train_subconjuntos[0], y_train_subconjuntos[0])\n",
        "  random_forest_model1.fit(X_train_subconjuntos[1], y_train_subconjuntos[1])\n",
        "  random_forest_model2.fit(X_train_subconjuntos[2], y_train_subconjuntos[2])\n",
        "  random_forest_model3.fit(X_train_subconjuntos[3], y_train_subconjuntos[3])\n",
        "  random_forest_modelos.append(random_forest_model)\n",
        "  random_forest_modelos.append(random_forest_model1)\n",
        "  random_forest_modelos.append(random_forest_model2)\n",
        "  random_forest_modelos.append(random_forest_model3)\n",
        "\n",
        "xgb_params_variations = []\n",
        "while len(xgb_params_variations) < 15:\n",
        "    new_params_xgb = {\n",
        "        'n_estimators': np.random.choice([50, 100, 200]),\n",
        "        'learning_rate': np.random.choice([0.1, 0.01, 0.001]),\n",
        "        'max_depth': np.random.choice([3, 5, 7, 9])\n",
        "    }\n",
        "    if new_params_xgb not in xgb_params_variations:\n",
        "        xgb_params_variations.append(new_params_xgb)\n",
        "\n",
        "\n",
        "xgb_modelos = []\n",
        "for variation in xgb_params_variations:\n",
        "    model = XGBClassifier(**variation)\n",
        "    model1 = XGBClassifier(**variation)\n",
        "    model2 = XGBClassifier(**variation)\n",
        "    model3 = XGBClassifier(**variation)\n",
        "\n",
        "    model.fit(X_train_subconjuntos[0], y_train_subconjuntos[0])\n",
        "    model1.fit(X_train_subconjuntos[1], y_train_subconjuntos[1])\n",
        "    model2.fit(X_train_subconjuntos[2], y_train_subconjuntos[2])\n",
        "    model3.fit(X_train_subconjuntos[3], y_train_subconjuntos[3])\n",
        "\n",
        "    xgb_modelos.append(model)\n",
        "    xgb_modelos.append(model1)\n",
        "    xgb_modelos.append(model2)\n",
        "    xgb_modelos.append(model3)\n",
        "\n",
        "svm_params_variations = []\n",
        "while len(svm_params_variations) < 15:\n",
        "    svm_params = {\n",
        "        'C': np.random.choice([0.1, 1.0, 10.0]),\n",
        "        'kernel': np.random.choice(['linear', 'rbf', 'poly']),\n",
        "        'degree': np.random.choice([2, 3, 4]),\n",
        "    }\n",
        "    if svm_params not in svm_params_variations:\n",
        "        svm_params_variations.append(svm_params)\n",
        "\n",
        "svm_modelos = []\n",
        "for variation in svm_params_variations:\n",
        "    model = SVC(**variation)\n",
        "    model1 = SVC(**variation)\n",
        "    model2 = SVC(**variation)\n",
        "    model3 = SVC(**variation)\n",
        "\n",
        "    model.fit(X_train_subconjuntos[0], y_train_subconjuntos[0])\n",
        "    model1.fit(X_train_subconjuntos[1], y_train_subconjuntos[1])\n",
        "    model2.fit(X_train_subconjuntos[2], y_train_subconjuntos[2])\n",
        "    model3.fit(X_train_subconjuntos[3], y_train_subconjuntos[3])\n",
        "\n",
        "    svm_modelos.append(model)\n",
        "    svm_modelos.append(model1)\n",
        "    svm_modelos.append(model2)\n",
        "    svm_modelos.append(model3)\n",
        "\n",
        "\n",
        "decision_tree_params_variations = []\n",
        "while len(decision_tree_params_variations) < 15:\n",
        "    decision_tree_params = {\n",
        "      'max_depth': np.random.choice([None, 5, 10]),\n",
        "      'min_samples_split': np.random.choice([2, 5, 10]),\n",
        "      'min_samples_leaf': np.random.choice([1, 2, 4])\n",
        "    }\n",
        "    if decision_tree_params not in decision_tree_params_variations:\n",
        "        decision_tree_params_variations.append(decision_tree_params)\n",
        "\n",
        "decision_tree_modelos = []\n",
        "for variation in decision_tree_params_variations:\n",
        "    model = DecisionTreeClassifier(**variation)\n",
        "    model1 = DecisionTreeClassifier(**variation)\n",
        "    model2 = DecisionTreeClassifier(**variation)\n",
        "    model3 = DecisionTreeClassifier(**variation)\n",
        "\n",
        "    model.fit(X_train_subconjuntos[0], y_train_subconjuntos[0])\n",
        "    model1.fit(X_train_subconjuntos[1], y_train_subconjuntos[1])\n",
        "    model2.fit(X_train_subconjuntos[2], y_train_subconjuntos[2])\n",
        "    model3.fit(X_train_subconjuntos[3], y_train_subconjuntos[3])\n",
        "\n",
        "    decision_tree_modelos.append(model)\n",
        "    decision_tree_modelos.append(model1)\n",
        "    decision_tree_modelos.append(model2)\n",
        "    decision_tree_modelos.append(model3)\n",
        "\n",
        "# print(len(random_forest_modelos))\n",
        "# print(len(xgb_modelos))\n",
        "# print(len(svm_regression_modelos))\n",
        "# print(len(decision_tree_modelos))\n",
        "\n",
        "\n",
        "\n",
        "model_accuracies = []\n",
        "# Loop para aplicar cada modelo aos dados de teste e obter a acurácia\n",
        "for model in random_forest_modelos + xgb_modelos + svm_modelos + decision_tree_modelos:\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    model_accuracies.append(accuracy)\n",
        "\n",
        "top_indices = sorted(range(len(model_accuracies)), key=lambda i: model_accuracies[i], reverse=True)[:24]\n",
        "\n",
        "modelos_melhores = []\n",
        "\n",
        "modelos_lista = []\n",
        "\n",
        "for i in top_indices:\n",
        "    if i < len(random_forest_modelos):\n",
        "        model_name = 'Foresta aleatoria'\n",
        "        model = random_forest_modelos[i]\n",
        "    elif i < len(random_forest_modelos) + len(xgb_modelos):\n",
        "        model_name = 'Xgb'\n",
        "        model = xgb_modelos[i - len(random_forest_modelos)]\n",
        "    elif i < len(random_forest_modelos) + len(xgb_modelos) + len(svm_modelos):\n",
        "        model_name = 'SVM'\n",
        "        model = decision_tree_modelos[i - len(random_forest_modelos) - len(xgb_modelos)]\n",
        "    else:\n",
        "        model_name = 'Arvore decisão'\n",
        "        model = decision_tree_modelos[i - len(random_forest_modelos) - len(xgb_modelos) - len(svm_modelos)]\n",
        "\n",
        "    modelos_melhores.append((model_name, model_accuracies[i]))\n",
        "    modelos_lista.append(model)\n",
        "\n",
        "#printa os modelos\n",
        "print(\"Modelos selecionados:\")\n",
        "for model_name, accuracy in modelos_melhores:\n",
        "    print(f\"{model_name} : {accuracy:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import mode\n",
        "\n",
        "# Supondo que você tem 24 modelos em um vetor chamado modelos_lista\n",
        "# E X_teste é o grupo de teste em que deseja fazer previsões\n",
        "# E y_teste é o vetor de valores reais correspondentes ao grupo de teste X_teste\n",
        "\n",
        "# Inicializar uma matriz para armazenar as previsões de cada modelo\n",
        "previsoes_modelos = np.zeros((len(modelos_lista), len(X_test)))\n",
        "\n",
        "# Fazer previsões para cada modelo em X_teste\n",
        "for i, modelo in enumerate(modelos_lista):\n",
        "    previsoes_modelos[i] = modelo.predict(X_test)\n",
        "\n",
        "# Calcular a moda das previsões de todos os modelos para obter a previsão final\n",
        "previsao_final, _ = mode(previsoes_modelos, axis=0, keepdims=True)\n",
        "\n",
        "# Converte a matriz resultante da moda em um array 1D\n",
        "previsao_final = previsao_final.flatten()\n",
        "\n",
        "# Agora você tem a previsão final do ensemble usando a moda para o grupo de teste X_teste\n",
        "\n",
        "# Compare as previsões finais com os valores reais em y_teste\n",
        "acuracia_ensemble = np.mean(previsao_final == y_test)\n",
        "print(\"Acurácia do ensemble usando a moda:\", acuracia_ensemble)\n"
      ],
      "metadata": {
        "id": "5pzereEg1r5b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a97b8912-ad07-4b6e-90e9-3c6e453596cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acurácia do ensemble usando a moda: 0.9707602339181286\n"
          ]
        }
      ]
    }
  ]
}